{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9d50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1701914199.708232       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 83), renderer: Apple M2\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "from time import time\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from imutils import face_utils\n",
    "from threading import Thread\n",
    "from multiprocessing import Process\n",
    "\n",
    "def process_video(frame, output_file):\n",
    "    shape_x = 48\n",
    "    shape_y = 48\n",
    "\n",
    "    def eye_aspect_ratio(eye):\n",
    "        A = distance.euclidean(eye[1], eye[5])\n",
    "        B = distance.euclidean(eye[2], eye[4])\n",
    "        C = distance.euclidean(eye[0], eye[3])\n",
    "        ear = (A + B) / (2.0 * C)\n",
    "        return ear\n",
    "\n",
    "    def detect_face(frame):\n",
    "        # Cascade classifier pre-trained model\n",
    "        cascPath = '/Users/prithvika/Downloads/face_landmarks.dat'\n",
    "        faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "        # BGR -> Gray conversion\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Cascade MultiScale classifier\n",
    "        detected_faces = faceCascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=6,\n",
    "                                                      minSize=(shape_x, shape_y),\n",
    "                                                      flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        coord = []\n",
    "\n",
    "        for x, y, w, h in detected_faces:\n",
    "            if w > 100:\n",
    "                sub_img = frame[y:y + h, x:x + w]\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 255), 1)\n",
    "                coord.append([x, y, w, h])\n",
    "\n",
    "        return gray, detected_faces, coord\n",
    "\n",
    "    def extract_face_features(faces, offset_coefficients=(0.075, 0.05)):\n",
    "        gray = faces[0]\n",
    "        detected_face = faces[1]\n",
    "\n",
    "        new_face = []\n",
    "\n",
    "        for det in detected_face:\n",
    "            # Regions of the face are detected\n",
    "            x, y, w, h = det\n",
    "            # a and y correspond to the gray conversion wheras w corresponds to the height\n",
    "\n",
    "            # Offset coefficient, np.floor takes the lowest integer (delete border of the image)\n",
    "            horizontal_offset = np.int(np.floor(offset_coefficients[0] * w))\n",
    "            vertical_offset = np.int(np.floor(offset_coefficients[1] * h))\n",
    "\n",
    "            # gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            # image is transformed to gray\n",
    "            extracted_face = gray[y + vertical_offset:y + h, x + horizontal_offset:x - horizontal_offset + w]\n",
    "\n",
    "            # Zoom on the extracted face\n",
    "            new_extracted_face = zoom(extracted_face, (shape_x / extracted_face.shape[0], shape_y / extracted_face.shape[1]))\n",
    "            # cast type float\n",
    "            new_extracted_face = new_extracted_face.astype(np.float32)\n",
    "            # scale\n",
    "            new_extracted_face /= float(new_extracted_face.max())\n",
    "            # print(new_extracted_face)\n",
    "\n",
    "            new_face.append(new_extracted_face)\n",
    "\n",
    "        return new_face\n",
    "\n",
    "    #using the cascade model, the facial features are recognized\n",
    "    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "    (nStart, nEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"nose\"]\n",
    "    (mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"mouth\"]\n",
    "    (jStart, jEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"jaw\"]\n",
    "\n",
    "    (eblStart, eblEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eyebrow\"]\n",
    "    (ebrStart, ebrEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eyebrow\"]\n",
    "\n",
    "    # keras model is loaded, this model is for videos\n",
    "    model = load_model('/Users/prithvika/Downloads/video.h5')\n",
    "    face_detect = dlib.get_frontal_face_detector()\n",
    "    predictor_landmarks = dlib.shape_predictor(\"/Users/prithvika/Downloads/face_landmarks.dat\")\n",
    "\n",
    "    # input the captured video\n",
    "    video_capture = cv2.VideoCapture(frame)\n",
    "    out = cv2.VideoWriter(output_file, cv2.VideoWriter_fourcc(*'mp4v'), 20.0, (640, 480))\n",
    "\n",
    "    start_time = time()\n",
    "    emotion_start_time = time()\n",
    "\n",
    "    emotion_durations = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}\n",
    "    max_emotion_duration = 0\n",
    "    max_emotion = None\n",
    "\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            # Break the loop if the video is finished\n",
    "            break\n",
    "\n",
    "        face_index = 0\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        rects = face_detect(gray, 1)\n",
    "\n",
    "        for (i, rect) in enumerate(rects):\n",
    "            shape = predictor_landmarks(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "            # Identify face coordinates\n",
    "            (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "            face = gray[y:y + h, x:x + w]\n",
    "\n",
    "            # Zoom on extracted face\n",
    "            face = zoom(face, (shape_x / face.shape[0], shape_y / face.shape[1]))\n",
    "\n",
    "            # Cast type float\n",
    "            face = face.astype(np.float32)\n",
    "\n",
    "            # Scale\n",
    "            face /= float(face.max())\n",
    "            face = np.reshape(face.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "            # Make Prediction\n",
    "            prediction = model.predict(face)\n",
    "            prediction_result = np.argmax(prediction)\n",
    "\n",
    "            # Rectangle around the face\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            cv2.putText(frame, \"Face #{}\".format(i + 1), (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                        (0, 255, 0), 2)\n",
    "\n",
    "            for (j, k) in shape:\n",
    "                cv2.circle(frame, (j, k), 1, (0, 0, 255), -1)\n",
    "\n",
    "            # 1. Add prediction probabilities\n",
    "            cv2.putText(frame, \"----------------\", (40, 100 + 180 * i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n",
    "            cv2.putText(frame, \"Emotional report : Face #\" + str(i + 1), (40, 120 + 180 * i),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n",
    "            cv2.putText(frame, \"Angry : \" + str(round(prediction[0][0], 3)), (40, 140 + 180 * i),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n",
    "            cv2.putText(frame, \"Disgust : \" + str(round(prediction[0][1], 3)), (40, 160 + 180 * i),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n",
    "            cv2.putText(frame, \"Fear : \" + str(round(prediction[0][2], 3)), (40, 180 + 180 * i),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "            cv2.putText(frame, \"Happy : \" + str(round(prediction[0][3], 3)), (40, 200 + 180 * i),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "            cv2.putText(frame, \"Sad : \" + str(round(prediction[0][4], 3)), (40, 220 + 180 * i),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "            cv2.putText(frame, \"Surprise : \" + str(round(prediction[0][5], 3)), (40, 240 + 180 * i),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "            cv2.putText(frame, \"Neutral : \" + str(round(prediction[0][6], 3)), (40, 260 + 180 * i),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "\n",
    "            # Calculate and print duration of each emotion\n",
    "            current_time = time()\n",
    "            emotion_duration = current_time - emotion_start_time\n",
    "#             print(f\"Duration of Emotion #{prediction_result + 1}: {emotion_duration} seconds\")\n",
    "\n",
    "            # Update emotion start time\n",
    "            emotion_start_time = current_time\n",
    "\n",
    "            # Add emotion duration to the cumulative total\n",
    "            emotion_durations[prediction_result] += emotion_duration\n",
    "\n",
    "            # 2. Annotate main image with a label\n",
    "            if prediction_result == 0:\n",
    "                cv2.putText(frame, \"Angry\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            elif prediction_result == 1:\n",
    "                cv2.putText(frame, \"Confusion\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            elif prediction_result == 2:\n",
    "                cv2.putText(frame, \"Fear\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            elif prediction_result == 3:\n",
    "                cv2.putText(frame, \"Happy\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            elif prediction_result == 4:\n",
    "                cv2.putText(frame, \"Sad\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            elif prediction_result == 5:\n",
    "                cv2.putText(frame, \"Surprise\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(frame, \"Neutral\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.putText(frame, 'Number of Faces : ' + str(len(rects)), (40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, 155, 1)\n",
    "        cv2.imshow('Video', frame)\n",
    "        out.write(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Calculate and print total duration of the video\n",
    "    total_duration = time() - start_time\n",
    "    print(f\"Total Duration of the Video: {total_duration} seconds\")\n",
    "\n",
    "    print(f\"Output for Emotion-Detection\")\n",
    "    # Print cumulative duration of each emotion\n",
    "    for emotion, duration in emotion_durations.items():\n",
    "#         print(f\"Duration of Emotion #{emotion + 1} (seconds): {duration}\")\n",
    "        if (emotion == 0):\n",
    "            print(f\"Duration of Anger is (seconds): {duration}\")\n",
    "        elif (emotion == 1):\n",
    "            print(f\"Duration of Confusion is (seconds): {duration}\")\n",
    "        elif (emotion == 2):\n",
    "            print(f\"Duration of Fear is (seconds): {duration}\")\n",
    "        elif (emotion == 3):\n",
    "            print(f\"Duration of Happiness is (seconds): {duration}\")\n",
    "        elif (emotion == 4):\n",
    "            print(f\"Duration of Sadness is (seconds): {duration}\")\n",
    "        elif (emotion == 5):\n",
    "            print(f\"Duration of Surprise is (seconds): {duration}\")\n",
    "        elif (emotion == 6):\n",
    "            print(f\"Duration of Neutral is (seconds): {duration}\")\n",
    "        else:\n",
    "            print(\"\")\n",
    "\n",
    "    # Find emotion with the maximum duration\n",
    "    max_emotion = max(emotion_durations, key=emotion_durations.get)\n",
    "    if (max_emotion == 0):\n",
    "        print(f\"The emotion that was observed most is: Anger\")\n",
    "    elif (max_emotion == 1):\n",
    "        print(f\"The emotion that was observed most is: Confusion\")\n",
    "    elif (max_emotion == 2):\n",
    "        print(f\"The emotion that was observed most is: Fear\")\n",
    "    elif (max_emotion == 3):\n",
    "        print(f\"The emotion that was observed most is: Happiness\")\n",
    "    elif (max_emotion == 4):\n",
    "        print(f\"The emotion that was observed most is: Sadness\")\n",
    "    elif (max_emotion == 5):\n",
    "        print(f\"The emotion that was observed most is: Surprise\")\n",
    "    elif (max_emotion == 6):\n",
    "        print(f\"The emotion that was observed most is: Neutral\")\n",
    "    else:\n",
    "        print(\"\")\n",
    "#     print(f\"The emotion that was observed most is: Emotion #{max_emotion + 1}\")\n",
    "\n",
    "    # When everything is done, release the capture\n",
    "    video_capture.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "def face_position(input_file):\n",
    "    #----------Nour's code-----------  \n",
    "    \n",
    "    official_start_time = time()\n",
    "    start_time = time()\n",
    "    end_time = 0\n",
    "\n",
    " # Variables to track time spent in different head pose directions\n",
    "    time_forward_seconds = 0\n",
    "    time_left_seconds = 0\n",
    "    time_right_seconds = 0\n",
    "    time_up_seconds = 0\n",
    "    time_down_seconds = 0\n",
    "    \n",
    "    \n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Specify the path to your video file\n",
    "    #video_path = input_file\n",
    "\n",
    "    cap = cv2.VideoCapture(input_file)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "\n",
    "        if not success:\n",
    "            print(\"Failed to read frame\")\n",
    "            break\n",
    "\n",
    "    # Flip the image horizontally for a later selfie-view display\n",
    "    # Also convert the color space from BGR to RGB\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # To improve performance\n",
    "        image.flags.writeable = False\n",
    "    \n",
    "    # Get the result\n",
    "        results = face_mesh.process(image)\n",
    "    \n",
    "    # To improve performance\n",
    "        image.flags.writeable = True\n",
    "    \n",
    "    # Convert the color space from RGB to BGR\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        img_h, img_w, img_c = image.shape\n",
    "        face_3d = []\n",
    "        face_2d = []\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                    if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                        if idx == 1:\n",
    "                            nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                            nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "\n",
    "                        x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "                        \n",
    "                        face_2d.append([x, y])\n",
    "                        face_3d.append([x, y, lm.z])       \n",
    "            \n",
    "                face_2d = np.array(face_2d, dtype=np.float64)\n",
    "                face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "            # The camera matrix\n",
    "                focal_length = 1 * img_w\n",
    "\n",
    "                cam_matrix = np.array([ [focal_length, 0, img_h / 2],\n",
    "                                        [0, focal_length, img_w / 2],\n",
    "                                        [0, 0, 1]])\n",
    "\n",
    "            # The Distance Matrix\n",
    "                dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "            # Solve PnP\n",
    "                success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "            # Get rotational matrix\n",
    "                rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "            # Get angles\n",
    "                angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "            # Print the rotation angles for debugging\n",
    "                #print(f\"X Rotation: {angles[0]*10000}\")\n",
    "                #print(f\"Y Rotation: {angles[1]*10000}\")\n",
    "\n",
    "            # See where the user's head is tilting\n",
    "                if angles[1]*10000 < -200:\n",
    "                    text = \"Looking Left\"\n",
    "                    time_left_seconds += 1 / fps\n",
    "#                     time_left_seconds += time.time() - start_time\n",
    "#                     start_time = time.time()\n",
    "                elif angles[1]*10000 > 200:\n",
    "                    text = \"Looking Right\"\n",
    "                    time_right_seconds += 1 / fps\n",
    "                elif angles[0]*10000 < -150:\n",
    "                    text = \"Looking Down\"\n",
    "                    time_down_seconds += 1 / fps\n",
    "                elif angles[0]*10000 > 350:\n",
    "                    text=\"Looking Up\"\n",
    "                    time_up_seconds += 1 / fps\n",
    "                else:\n",
    "                    text = \"Forward\"\n",
    "                    time_forward_seconds += 1 / fps\n",
    "\n",
    "            # Display the nose direction\n",
    "                nose_3d_projection, jacobian = cv2.projectPoints(nose_3d, rot_vec, trans_vec, cam_matrix, dist_matrix)\n",
    "\n",
    "                p1 = (int(nose_2d[0]), int(nose_2d[1]))\n",
    "                p2 = (int(nose_3d_projection[0][0][0]), int(nose_3d_projection[0][0][1]))\n",
    "            \n",
    "                cv2.line(image, p1, p2, (255, 0, 0), 2)\n",
    "\n",
    "            # Add the text on the image\n",
    "                cv2.putText(image, text, (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        cv2.imshow('Head Pose Estimation', image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "#cv2.destroyAllWindows()\n",
    "    end_time = time()\n",
    "    elapsed_time_minutes = (end_time - official_start_time)/60\n",
    "    \n",
    "    print(f\"Output for Head-Pose-Detection\")\n",
    "    print(f\"Duration of Time Looking Forward: {time_forward_seconds} seconds\")\n",
    "    print(f\"Duration of Time Looking Left: {time_left_seconds} seconds\")\n",
    "    print(f\"Duration of Time Looking Right: {time_right_seconds} seconds\")\n",
    "    print(f\"Duration of Time Looking Up: {time_up_seconds} seconds\")\n",
    "    print(f\"Duration of Time Looking Down: {time_down_seconds} seconds\")\n",
    "    \n",
    "    total_video_duration = end_time - official_start_time\n",
    "    \n",
    "    max_pose = max(time_forward_seconds, time_left_seconds, time_up_seconds, time_right_seconds, time_down_seconds)\n",
    "    \n",
    "    if max_pose == time_forward_seconds:\n",
    "        print(\"The most observed head-pose is: Looking Forward\")\n",
    "    elif max_pose == time_left_seconds:\n",
    "        print(\"The most observed head-pose is: Looking Left\")\n",
    "    elif max_pose == time_right_seconds:\n",
    "        print(\"The most observed head-pose is: Looking Right\")\n",
    "    elif max_pose == time_up_seconds:\n",
    "        print(\"The most observed head-pose is: Looking Up\")\n",
    "    else:\n",
    "        print(\"The most observed head-pose is: Looking Down\")\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def run_cpu_tasks_in_parallel(tasks):\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        running_tasks = [executor.submit(task) for task in tasks]\n",
    "        for running_task in running_tasks:\n",
    "            running_task.result()\n",
    "#---------------------------------\n",
    "\n",
    "def eye_tracking(input_file):\n",
    "    #-------------Dhawni's code----------------\n",
    "    def detect_eyes(frame):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = detector(gray)\n",
    "\n",
    "        if faces:\n",
    "            shape = predictor(gray, faces[0])\n",
    "            left_eye = shape.parts()[36:42]\n",
    "            right_eye = shape.parts()[42:48]\n",
    "            return left_eye, right_eye\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "    def calculate_ear(eye):\n",
    "        eye = np.array([(point.x, point.y) for point in eye])\n",
    "        A = np.linalg.norm(eye[1] - eye[5])\n",
    "        B = np.linalg.norm(eye[2] - eye[4])\n",
    "        C = np.linalg.norm(eye[0] - eye[3])\n",
    "        ear = (A + B) / (2.0 * C)\n",
    "        return ear\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\"/Users/prithvika/Downloads/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Initialize video capture (replace with your video file path)\n",
    "    video_path = input_file  # Replace with your video file path\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties for the output video\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#output_video = cv2.VideoWriter('/content/output_video.avi', fourcc, fps, (width, height))\n",
    "\n",
    "    # Initialize variables to record durations\n",
    "    duration_eyes_closed = 0\n",
    "    duration_looking_left = 0\n",
    "    duration_looking_right = 0\n",
    "    duration_looking_straight = 0\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "    count_left = 0\n",
    "    count_right = 0\n",
    "    count_straight = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        left_eye, right_eye = detect_eyes(frame)\n",
    "\n",
    "        if left_eye is not None and right_eye is not None:\n",
    "            ear_left = calculate_ear(left_eye)\n",
    "            ear_right = calculate_ear(right_eye)\n",
    "\n",
    "        # Calculate the average EAR for both eyes\n",
    "            avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "        # Set a threshold for distraction detection (you may need to adjust this)\n",
    "            distraction_threshold = 0.2\n",
    "\n",
    "        # Check if the person is distracted\n",
    "            if avg_ear < distraction_threshold:\n",
    "                cv2.putText(frame, \"Eyes Closed\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "                duration_eyes_closed += 1 / fps\n",
    "                count_straight += 1\n",
    "            else:\n",
    "            # Check gaze direction\n",
    "                horizontal_ratio = (left_eye[0].x + right_eye[3].x) / 2 / width\n",
    "                if horizontal_ratio < 0.4:\n",
    "                    cv2.putText(frame, \"Looking Left\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "                    duration_looking_left += 1 / fps\n",
    "                    count_left += 1\n",
    "                elif horizontal_ratio > 0.6:\n",
    "                    cv2.putText(frame, \"Looking Right\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "                    duration_looking_right += 1 / fps\n",
    "                    count_right += 1\n",
    "                else:\n",
    "                    cv2.putText(frame, \"Looking Straight\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "                    duration_looking_straight += 1 / fps\n",
    "\n",
    "        # Draw contours around eyes\n",
    "            for eye in [left_eye, right_eye]:\n",
    "                for point in eye:\n",
    "                    x, y = point.x, point.y\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "   # output_video.write(frame)  # Write the frame to the output video\n",
    "\n",
    "    #cv2_imshow(frame)  # Use cv2_imshow instead of cv2.imshow\n",
    "        cv2.imshow('Frame', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture object, video writer, and close all windows\n",
    "    cap.release()\n",
    "    print(f\"Output for Eye-Tracking\")\n",
    "    print(f\"Duration taken looking right: {duration_looking_right} sec\")\n",
    "    print(f\"Duration taken looking left: {duration_looking_left} sec\")\n",
    "    print(f\"Duration taken closed eyes: {duration_eyes_closed} sec\")\n",
    "    print(f\"Duration taken looking straight: {duration_looking_straight} sec\")\n",
    "    \n",
    "    max_duration = max(duration_looking_right, duration_looking_left, duration_looking_straight, duration_eyes_closed)\n",
    "    if max_duration == duration_looking_right:\n",
    "        print(\"Most observed eye movement is: Looking Right\")\n",
    "    elif max_duration == duration_looking_left:\n",
    "        print(\"Most observed eye movement is: Looking Left\")\n",
    "    elif max_duration == duration_looking_straight:\n",
    "        print(\"Most observed eye movement is: Looking Straight\")\n",
    "    else:\n",
    "        print(\"Most observed eye movement is: Eyes Closed\")\n",
    "#output_video.release()\n",
    "    #cv2.destroyAllWindows()\n",
    "    #-------------------------------------------\n",
    "    \n",
    "\n",
    "    \n",
    "def display_on_same_window(frame, text1, text2, text3):\n",
    "    cv2.putText(frame, text1, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "    cv2.putText(frame, text2, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "    cv2.putText(frame, text3, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "    cv2.imshow('Combined Output', frame)\n",
    "\n",
    "def main():\n",
    "    input_video_file = '/Users/prithvika/Downloads/Emotion Videos/WhatsApp Video 2023-11-26 at 20.32.15.mp4'\n",
    "    output_video_file = '/Users/prithvika/Downloads/output_video.mp4'\n",
    "\n",
    "    original_stdout = sys.stdout\n",
    "    with open('/Users/prithvika/Downloads/output.txt', 'w') as f:\n",
    "    # Redirect standard output to the file\n",
    "        sys.stdout = f\n",
    "    \n",
    "    #process_video(input_video_file, output_video_file)\n",
    "        print(f\"New Instance:\")\n",
    "        run_cpu_tasks_in_parallel([\n",
    "            process_video(input_video_file, output_video_file),\n",
    "            face_position(input_video_file),\n",
    "            eye_tracking(input_video_file),\n",
    "        ])\n",
    "    \n",
    "        sys.stdout = original_stdout\n",
    "    \n",
    "    #Thread(target = process_video(input_video_file, output_video_file)).start()\n",
    "    #Thread(target = face_position(input_video_file)).start()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a9bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
